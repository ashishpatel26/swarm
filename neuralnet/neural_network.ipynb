{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "* possibly add other activation functions (non-differentiable)\n",
    "* simulated annealing? ~~nelder mead?~~\n",
    "* ~~right now, only globalbestpso used~~\n",
    "* ~~add button to run optimization only after clicking~~\n",
    "* ~~Cost history / number of iterations needed compared to classic MLP~~ comparison between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are a way of parametrizing non-linear functions. On a very basic level, they are formed by a composition of non-linear function. The function is defined with a layered architecture. The mapping from the input layer to the output layer is performed via hidden layers. Each layer $k$ produces an output $z_k$ that is a non-linear function of a weighted combination of the outputs of the previous layer, $z_k = g_k(W_k z_{k-1})$. \n",
    "\n",
    "Once the architecture and the activation functions $g_k(\\cdot)$ are defined, the weights $W_k$ are trained. If all the functions $g_k$ are (sub)-differentiable then, via the chain rule, gradients exist and can be computed. Usually, the weights are then trained via different variants of gradient descent.\n",
    "\n",
    "What we do here in this notebook is another approach: instead of using a gradient based method that does the fitting of the network, we want to apply the particle swarm optimization, simulated annealing and Nelder Mead algorithms. We will use sample datasets for binary classification that are supplied by Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed\n",
    "from IPython.display import clear_output\n",
    "from timeit import default_timer as timer\n",
    "from scipy.optimize import minimize as scipy_minimize\n",
    "\n",
    "from utilities import plot_helpers\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (10, 5)  # Change this if figures look ugly. \n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pyswarms as ps\n",
    "\n",
    "def get_dimensions(hidden_layer_sizes, n_features, n_classes):\n",
    "    dimensions = 0\n",
    "    in_i = n_features\n",
    "    for l in hidden_layer_sizes:\n",
    "        dimensions += in_i * l + l\n",
    "        in_i = l\n",
    "    dimensions += in_i * n_classes + n_classes\n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Since the PySwarm optimizer needs a specific objective function, we implement a basic neural network with that works with different activation functions and variable hidden layers. This implementation is based on NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid aka. Logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# Relu function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# tanh\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def inv(x):\n",
    "    return np.sqrt(np.abs(x))\n",
    "\n",
    "activations = {\n",
    "    \"logistic\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"identity\": lambda x: x,\n",
    "    \"tanh\": tanh,\n",
    "    \"inv\": inv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 200\n",
    "N_FEATURES = 2\n",
    "N_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Prop, Loss and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(p, activation, n_inputs, n_classes, hidden_layer_sizes, X):\n",
    "    \"\"\" Calculate roll-back the weights and biases\n",
    "    Inputs\n",
    "    ------\n",
    "    p: np.ndarray\n",
    "        unrolled version of the weights and biases\n",
    "        \n",
    "    activation: function :: np.ndarray -> np.ndarray\n",
    "  \n",
    "    hidden_layer_sizes: tuple (int)\n",
    "  \n",
    "    X: np.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of logits for the output layer, which correspond\n",
    "    to the probabilities of predicting each class\n",
    "\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    in_i = X\n",
    "    for i in range(0, len(hidden_layer_sizes)):\n",
    "        layer_size = hidden_layer_sizes[i]\n",
    "        no_weights = n_inputs * layer_size\n",
    "        # get weights and biases for this layer\n",
    "        W_i = p[start:(start+no_weights)].reshape((n_inputs, layer_size))\n",
    "        b_i = p[(start+no_weights):(start+no_weights+layer_size)].reshape((layer_size,))\n",
    "        \n",
    "        out_i = activation(in_i.dot(W_i) + b_i)\n",
    "        \n",
    "        # update variables\n",
    "        start = start + no_weights + layer_size\n",
    "        n_inputs = layer_size\n",
    "        in_i = out_i\n",
    "    \n",
    "    # Compute last layer\n",
    "    no_weights = n_inputs * n_classes\n",
    "    W_k = p[start:(start+no_weights)].reshape((n_inputs, n_classes))\n",
    "    b_k = p[(start+no_weights):(start+no_weights+n_classes)].reshape((n_classes,))\n",
    "    \n",
    "    # Pre-activation\n",
    "    out = in_i.dot(W_k) + b_k\n",
    "    \n",
    "    # Probabilities using softmax\n",
    "    # make every value 0 or below, as exp(0) won't overflow\n",
    "    out_scaled = out - out.max(axis=-1, keepdims=True)\n",
    "    exp_scores = np.exp(out_scaled)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the loss of our current network, using negative log likelihood\n",
    "def loss(params, activation, n_inputs, n_classes, hidden_layer_sizes, reg, X, Y):\n",
    "    probs = forward_prop(params, activation, n_inputs, n_classes, hidden_layer_sizes, X)\n",
    "    correct_logprobs = -np.log(probs[range(X.shape[0]), Y])\n",
    "    loss = np.sum(correct_logprobs) / X.shape[0]\n",
    "    return loss + reg * np.linalg.norm(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using the probabilities of our forward propagation\n",
    "def predict(params, activation, n_inputs, n_classes, hidden_layer_sizes, X):\n",
    "    probs = forward_prop(params, activation, n_inputs, n_classes, hidden_layer_sizes, X)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Demo\n",
    "\n",
    "Neural network training has a lot of hyperparameters. Architecture, learning rate, batch size, optimization algorithm, random seed are just a few of them. Additionally, we have the hyperparameters for the swarm optimization. These generally are:\n",
    "* $c_1$, the cognitive parameter (attraction towards individual best)\n",
    "* $c_2$, the social parameter (attraction towards global/neighborhood best)\n",
    "* $w$, the inertia or momentum\n",
    "\n",
    "Also, the topology can be specified when using the local optimization method (using neighborhood best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_creation(dataset, noise):\n",
    "    if dataset is 'blobs':\n",
    "        X, Y = datasets.make_blobs(n_samples=N_SAMPLES, centers=2, random_state=3, cluster_std=10*noise)\n",
    "    elif dataset is 'circles':\n",
    "        X, Y = datasets.make_circles(n_samples=N_SAMPLES, factor=.5, noise=noise, random_state=42)\n",
    "    elif dataset is 'moons':\n",
    "        X, Y = datasets.make_moons(n_samples=N_SAMPLES, noise=noise, random_state=42)\n",
    "    elif dataset == 'xor':\n",
    "        np.random.seed(42)\n",
    "        step = int(N_SAMPLES/4)\n",
    "        \n",
    "        X = np.zeros((N_SAMPLES, 2))\n",
    "        Y = np.zeros(N_SAMPLES)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([1, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = 0\n",
    "        X[3*step:4*step, :] = np.array([1, 0]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = 0\n",
    "    \n",
    "    elif dataset == 'periodic':\n",
    "        \n",
    "        step = int(N_SAMPLES/4)\n",
    "        \n",
    "        X = np.zeros((N_SAMPLES, 2))\n",
    "        Y = np.zeros(N_SAMPLES)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([0, 2]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = 0\n",
    "        X[3*step:4*step, :] = np.array([0, 3]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = 0\n",
    "    \n",
    "    \n",
    "    X = X[Y <= 1, :]\n",
    "    Y = Y[Y <=1 ]\n",
    "   # Y[Y==0] = -1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the interactive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (10, 5)  # Change this if figures look ugly. \n",
    "rcParams['font.size'] = 16\n",
    "def mlp(solver, k, dataset, hidden_layer_sizes, activation, iters, particles, c1, c2, w, reg, noise):\n",
    "    # constants for this example\n",
    "    N_FEATURES = 2\n",
    "    N_CLASSES = 2\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # timer to see time of execution\n",
    "    start = timer()\n",
    "    \n",
    "    # to calculate the dimension, we know we have (in * out) no. of weights\n",
    "    # and (out) no. of biases per layer\n",
    "    dimensions = get_dimensions(hidden_layer_sizes, N_FEATURES, N_CLASSES)\n",
    "    print(\"Dimensions for this problem:\", dimensions)\n",
    "    \n",
    "    # get the data\n",
    "    X, Y = data_creation(dataset, noise)\n",
    "    \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y.astype(int), test_size=.2)\n",
    "    \n",
    "    # get function to corresponding string\n",
    "    activation_function = activations[activation]\n",
    "    \n",
    "    # Pick optimizer and train\n",
    "    if solver == 'Nelder-Mead':\n",
    "        # wrap the function to use just x\n",
    "        f = lambda x: loss(x, activation_function, N_FEATURES, N_CLASSES,\\\n",
    "                    hidden_layer_sizes, np.power(10., reg), X_train, y_train)\n",
    "        \n",
    "        # guess randomly between -1 and 1\n",
    "        initial_guess = np.random.uniform(low=-1.0, high=1.0, size=(dimensions + 1, dimensions))\n",
    "        \n",
    "        result = scipy_minimize(fun=f, x0=initial_guess[0,:],\\\n",
    "                                method='Nelder-Mead', \\\n",
    "                                options={'adaptive': False,\n",
    "                                         'maxiter': iters,\n",
    "                                         'initial_simplex': initial_guess},\\\n",
    "                                callback=lambda xk: print('current loss: {}'.format(f(xk)), end=\"\\r\", flush = False)\n",
    "                               ) \n",
    "        \n",
    "        if not result.success:\n",
    "            print(\"\\nOptimizer exited unsuccessfully. Message:\", result.message)\n",
    "        print('Nelder-Mead done after', result.nit, \"iterations\\n\",\\\n",
    "             \"Loss value:\", result.fun)\n",
    "        pos = result.x\n",
    "        print(\"Validation accuracy:\", \\\n",
    "          (y_test == predict(pos, activation_function, N_FEATURES, N_CLASSES,\\\n",
    "                    hidden_layer_sizes, X_test)).mean())\n",
    "        \n",
    "    elif solver == 'sgd':\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    activation=activation, solver=solver, max_iter=iters, \n",
    "                    alpha=np.power(10., reg), random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        print(\"Validation accuracy: {}\".format(classifier.score(X_test, y_test)))\n",
    "    else:\n",
    "         # wrap our function for particles\n",
    "        def f(x):\n",
    "            n_particles = x.shape[0]\n",
    "            res = [loss(x[i], activation_function, N_FEATURES, N_CLASSES,\\\n",
    "                  hidden_layer_sizes, 10**reg, X_train, y_train)\\\n",
    "                       for i in range(n_particles)]\n",
    "            return np.array(res)\n",
    "        # The options for the optimizer\n",
    "        options = {'c1': c1, 'c2': c2, 'w': w, 'k': k, 'p': 2}\n",
    "    \n",
    "        if solver == 'GlobalBestPSO':\n",
    "            optimizer = ps.single.GlobalBestPSO(n_particles=particles, dimensions=dimensions,\\\n",
    "                                        options=options)\n",
    "        \n",
    "        elif solver == 'LocalBestPSO':        \n",
    "            optimizer = ps.single.LocalBestPSO(n_particles=particles, dimensions=dimensions,\\\n",
    "                                               options=options)\n",
    "    \n",
    "    \n",
    "        cost, pos = optimizer.optimize(f, iters=iters)\n",
    "    \n",
    "        print(\"Validation accuracy: {}\".format(\n",
    "              (y_test == predict(pos, activation_function, N_FEATURES, N_CLASSES,\n",
    "                        hidden_layer_sizes, X_test)).mean()))\n",
    "    \n",
    "    end = timer()\n",
    "    print(\"Elapsed time in seconds: {:3.5}\".format(end - start))\n",
    "    \n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    fig = plt.axes()\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 0)[0], 0], X[np.where(Y == 0)[0], 1], fig=fig, options=opt)\n",
    "\n",
    "    mins = np.min(X, 0)\n",
    "    maxs = np.max(X, 0)\n",
    "    x_min = mins[0] - 1\n",
    "    x_max = maxs[0] + 1\n",
    "    y_min = mins[1] - 1\n",
    "    y_max = maxs[1] + 1\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]  \n",
    "    Xplot = np.c_[XX.ravel(), YY.ravel()]   \n",
    "    \n",
    "    # get all the predictions\n",
    "    if solver == 'sgd':\n",
    "        Z = classifier.predict_proba(Xplot)[:, 1]\n",
    "    else:\n",
    "        Z = forward_prop(pos, activation_function, N_FEATURES, N_CLASSES, hidden_layer_sizes, Xplot)[:,1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    # plt.figure(fignum, figsize=(4, 3))\n",
    "    # Put the result into a color plot\n",
    "    plt.contourf(XX, YY, Z, cmap=plt.cm.jet, alpha=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive modelation\n",
    "\n",
    "To be able to visually see and play around with the different parameters, we here give an interactive tool. One can pick different hyperparameters, datasets, layers etc. and see the dataset as well as the decision boundaries of the trained weights and biases of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab8c3201c094bd2b9b58a76aa788e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Solver', options=('GlobalBestPSO', 'LocalBestPSO', 'Nelder-Mead', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver_widget = ipywidgets.Dropdown(options=['GlobalBestPSO', 'LocalBestPSO', 'Nelder-Mead', 'sgd'],\\\n",
    "                            value='GlobalBestPSO', description='Solver', disabled=False)\n",
    "\n",
    "particles_widget = ipywidgets.Dropdown(options=[20,50,100,200,1000],\\\n",
    "                               value=20, description='Particles', disabled=False)\n",
    "\n",
    "c1_widget = ipywidgets.FloatSlider(value=0.5, min=0, max=1, step=0.1, readout_format='.1f',\n",
    "                            description='c1:', style={'description_width': 'initial'},\n",
    "                            continuous_update=False)\n",
    "\n",
    "c2_widget = ipywidgets.FloatSlider(value=0.3,min=0,max=1,step=0.1,readout_format='.1f',\n",
    "                            description='c2:', style={'description_width': 'initial'},\n",
    "                            continuous_update=False)\n",
    "\n",
    "w_widget = ipywidgets.FloatSlider(value=0.9,min=0,max=1,step=0.1,readout_format='.1f',\n",
    "                           description='w:', style={'description_width': 'initial'},\n",
    "                           continuous_update=False)\n",
    "\n",
    "k_widget = ipywidgets.IntSlider(value=5,min=1, max=particles_widget.value, step=1,\n",
    "                            readout_format='d',description='neighborhood k:',\n",
    "                            style={'description_width': 'initial'},\n",
    "                            continuous_update=False, disabled=True)\n",
    "\n",
    "def disable_pso_args(*args):\n",
    "    # enable the neighborhood k only for Local PSO\n",
    "    k_widget.disabled = True if solver_widget.value != 'LocalBestPSO' else False\n",
    "    k_widget.max = particles_widget.value\n",
    "    # enable all PSO hyperparameters only for PSO\n",
    "    c1_widget.disabled = False if solver_widget.value in ['GlobalBestPSO', 'LocalBestPSO'] else True\n",
    "    c2_widget.disabled = False if solver_widget.value in ['GlobalBestPSO', 'LocalBestPSO'] else True\n",
    "    w_widget.disabled = False if solver_widget.value in ['GlobalBestPSO', 'LocalBestPSO'] else True\n",
    "    particles_widget.disabled = False if solver_widget.value in ['GlobalBestPSO', 'LocalBestPSO'] else True\n",
    "    \n",
    "solver_widget.observe(disable_pso_args)\n",
    "particles_widget.observe(disable_pso_args)\n",
    "\n",
    "interact_manual(mlp, \n",
    "        solver=solver_widget,\n",
    "        k=k_widget,\n",
    "        dataset=['blobs', 'circles', 'moons', 'xor', 'periodic'],\n",
    "        activation=['relu', 'logistic', 'identity', 'tanh'],\n",
    "        hidden_layer_sizes=[(50, ), (100, ), (50, 50), (100, 100), (50, 50, 50), (100, 100, 100)],\n",
    "        iters=[100,200,500,1000,2000,3000,5000,10000],\n",
    "        particles=particles_widget,\n",
    "        c1=c1_widget,\n",
    "        c2=c2_widget,\n",
    "        w=w_widget,\n",
    "        reg=ipywidgets.FloatSlider(value=-3,min=-3,max=3,step=0.1,readout_format='.1f',\n",
    "                    description='reg 10^:',style={'description_width': 'initial'},\n",
    "                    continuous_update=False),\n",
    "        noise=ipywidgets.FloatSlider(value=0.05,min=0.01,max=0.3,step=0.01,\n",
    "                    readout_format='.2f',description='noise:', style={'description_width': 'initial'},\n",
    "                    continuous_update=False),  \n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can see that the PSO works quite well to find the fit for the classification and judging by the visualization the decision boundaries found by the swarm is very much reasonable in almost all cases. Evidently, the underlying function we are trying to optimize is an extremely high-dimensional non-convex function with (in general) no single global optimum, but many local minimas or possible solutions.\n",
    "\n",
    "For Nelder-Mead, it does not seem to work as good as PSO and does not give as good classification boundaries as quickly. It needs a big amount of iterations with often negligible improvement despite being nowhere near to a 'nice' solution. In other words, if it gives a good solution it took NM a big amount of iterations. One way to interpret this is the dependency of NM to a good initialization as well as its little 'learning rate', i.e. improvement in one iteration. Also, there is the risk of being stuck in a local optimum. It does, however, seem to give similar results to SGD, just needs more time to execute.\n",
    "\n",
    "Note that the PSO initial guess is chosen uniformly at random between 0 and 1 for every dimension, where with NM we saw best results with sampling between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NM vs SGD](nm_sgd.png)\n",
    "*Circle Dataset and ReLU activation, one hidden layer*\n",
    " * Left: NM with 2000 iterations\n",
    " * Middle: NM with 5000 iterations\n",
    " * Right: SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the PSO and NM can give somewhat different results compared to popular optimization methods like SGD and Adam. In fact, using the logistic activation function for a linearly non-separable dataset like *moons*, classic SGD fails to find a good solution, whereas PSO does, and NM with a big amount of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![PSO vs. SGD vs. NM for logistic activation](logistic_pso_sgd_nm.png)\n",
    "*PSO vs. SGD vs. NM (10k iterations) for logistic activation and the moon dataset, one hidden layer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between GlobalBestPSO and LocalBestPSO, there are not immediately striking differences in the results and performances. One example, however, is the circle dataset, where a smaller neighborhood seems to give better decision boundaries:\n",
    "\n",
    "![GlobalBestPSO vs LocalBestPSO](globalpso_local10_local5.png)\n",
    "*Circle Dataset with ReLU activation, one hidden layer, 20 particles, 200 iterations*\n",
    " * Left: GlobalBestPSO\n",
    " * Middle: LocalBestPSO, 10 particle neighborhood\n",
    " * Right: LocalBestPSO, 5 particle neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "We already mentioned the slow convergence of NM and the need for many iterations.\n",
    "<br/> <br/>\n",
    "The biggest problem with PSO and neural network training certainly is the complexity. With just one hidden layer of size $50$, we have $252$ weights and biases to be adjusted, therefore a $252$-dimensional function which we then evaluate $50 * 200 = 10^4$ times for e.g. $50$ particles and $200$ iterations.\n",
    "<br/><br/>\n",
    "\n",
    "Let's consider this with a small excursion to the theoretical complexity of the forward propagation:\n",
    "\n",
    "Given one feature vector $x$, we first multiply the vector with the weight matrix $W_1$, and then add the bias $b_1$ before applying the activation function. Assuming the number of perceptrons in the hidden layer have the same dimension $n$ like the input, we have a complexity of $O(n^2)$ as a result of the matrix with vector multiplication. Repeating this for $k$ layers and $m$ data points, we get something in the order of $O(k*m*n^2)$. Repeating this operation for every particle for every iteration therefore is a big computational burden.\n",
    "<br/> <br/> <br/>\n",
    "Let's have a comparison for one specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5445f02a30546168f7b1827325a8c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options=('iris', 'breast cancer', 'wine'), value='iris')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.compare_nm_pso(dataset)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_nm_pso(dataset):\n",
    "    np.random.seed(42)\n",
    "    # hyperparameters and choices of variables\n",
    "    hidden_layer_sizes = (30,30)\n",
    "    activation_function = relu\n",
    "    reg = -3.0\n",
    "\n",
    "    # PSO specific\n",
    "    c1 = 0.5\n",
    "    c2 = 0.3\n",
    "    w = 0.9\n",
    "    n_particles = 20\n",
    "\n",
    "    # iters chosen to roughly have same number of function evaluations\n",
    "    iters_pso = 200\n",
    "    iters_nm = int(n_particles * iters_pso / 3)\n",
    "\n",
    "    # load data\n",
    "    if dataset == 'breast cancer':    \n",
    "        X, Y = datasets.load_breast_cancer(return_X_y=True)\n",
    "        N_CLASSES = 2\n",
    "    elif dataset == 'iris':\n",
    "        X, Y = datasets.load_iris(return_X_y=True)\n",
    "        N_CLASSES = 3\n",
    "    else:\n",
    "        X, Y = datasets.load_wine(return_X_y=True)\n",
    "        N_CLASSES = 3\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state=0)\n",
    "\n",
    "    N_FEATURES = len(X[0])\n",
    "\n",
    "    dimensions = get_dimensions(hidden_layer_sizes, N_FEATURES, N_CLASSES)\n",
    "    print(\"Dataset: {}, Features: {}, Samples: {}, Training Size: {}, Layers: {}\".format(\n",
    "        dataset, N_FEATURES, len(X), len(X_train), hidden_layer_sizes))\n",
    "    print(\"Dimensions for this problem: {}\".format(dimensions))\n",
    "    print(\"Method PSO: {} particles, {} iterations\".format(n_particles, iters_pso))\n",
    "    print(\"Method NM: {} iterations\".format(iters_nm))\n",
    "    \n",
    "    evals_hist = {\n",
    "        \"NM\": {\n",
    "            \"eval_count\": 0,\n",
    "            \"fbest\": 5.0,\n",
    "            \"fbest_hist\": []\n",
    "        },\n",
    "        \"PSO\": {\n",
    "            \"eval_count\": 0,\n",
    "            \"fbest\": 5.0,\n",
    "            \"fbest_hist\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # define a loss function where we keep track of the number of evaluations and the fbest\n",
    "    def loss_tracked(method, params, activation, hidden_layer_sizes, reg, X, Y):\n",
    "        res = loss(params, activation, N_FEATURES, N_CLASSES, hidden_layer_sizes, reg, X, Y)\n",
    "        evals_hist[method]['eval_count'] += 1\n",
    "        curr_fbest = evals_hist[method]['fbest']\n",
    "        evals_hist[method]['fbest'] = np.minimum(curr_fbest, res)\n",
    "        evals_hist[method]['fbest_hist'].append(evals_hist[method]['fbest'])\n",
    "        return res\n",
    "\n",
    "    # functions wrapping the arguments for both NM and PSO\n",
    "    def f_nm(x):\n",
    "        return loss_tracked(\"NM\", x, activation_function, hidden_layer_sizes,\\\n",
    "                            np.power(10., reg), X_train, y_train)\n",
    "    def f_pso(x):\n",
    "        n_particles = x.shape[0]\n",
    "        res = [loss_tracked(\"PSO\", x[i], activation_function, hidden_layer_sizes, 10**reg, X_train, y_train)\\\n",
    "               for i in range(n_particles)]\n",
    "        return np.array(res)\n",
    "\n",
    "    # first: Nelder-Mead\n",
    "    print(\"--------- Nelder-Mead\")\n",
    "    initial_guess = np.random.uniform(low=-1.0, high=1.0, size=(dimensions + 1, dimensions))\n",
    "    start = timer()\n",
    "    result = scipy_minimize(fun=f_nm, x0=initial_guess[0,:],\\\n",
    "                            method='Nelder-Mead', \\\n",
    "                            options={'adaptive': False,\n",
    "                                     'maxiter': iters_nm,\n",
    "                                     'initial_simplex': initial_guess}) \n",
    "\n",
    "    if not result.success:\n",
    "        print(\"\\nOptimizer exited unsuccessfully. Message:\", result.message)\n",
    "    print('Nelder-Mead done after {} evaluations\\nf_best: {}'.format(evals_hist['NM']['eval_count'],\n",
    "                                                                     evals_hist['NM']['fbest']), flush=True)\n",
    "    pos_nm = result.x\n",
    "    print(\"Validation accuracy:\", \\\n",
    "      (y_test == predict(pos_nm, activation_function,\\\n",
    "                         N_FEATURES, N_CLASSES, hidden_layer_sizes, X_test)).mean(), flush=True)\n",
    "    end = timer()\n",
    "    print(\"Elapsed time in seconds: {:3.5}\".format(end - start), flush=True)\n",
    "\n",
    "    # now: PSO\n",
    "    print(\"\\n--------- PSO\", flush=True)\n",
    "    start = timer()\n",
    "    options = {\"c1\": c1, \"c2\": c2, \"w\": w}\n",
    "    optimizer = ps.single.GlobalBestPSO(n_particles=n_particles, dimensions=dimensions,\\\n",
    "                                            options=options)\n",
    "    cost_pso, pos_pso = optimizer.optimize(f_pso, iters=iters_pso)\n",
    "    print('PSO done after {} evaluations\\nf_best: {}'.format(evals_hist['PSO']['eval_count'],\n",
    "                                                                     evals_hist['PSO']['fbest']))\n",
    "    print(\"Validation accuracy: {}\".format(\n",
    "          (y_test == predict(pos_pso, activation_function, N_FEATURES, N_CLASSES,\n",
    "                    hidden_layer_sizes, X_test)).mean()))\n",
    "    end = timer()\n",
    "    print(\"Elapsed time in seconds: {:3.5}\".format(end - start))\n",
    "\n",
    "    # ---- plotting\n",
    "    x_axis_pso = np.arange(1, 1 + evals_hist['PSO']['eval_count'], 1)\n",
    "    x_axis_nm = np.arange(1, 1 + evals_hist['NM']['eval_count'], 1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_axis_pso, evals_hist['PSO']['fbest_hist'], 'b-')\n",
    "    ax.plot(x_axis_nm, evals_hist['NM']['fbest_hist'], 'r-')\n",
    "    ax.legend(['PSO','NM'])\n",
    "    ax.set(xlabel='Number of function evaluations', ylabel='Best loss found',\\\n",
    "           title='NM vs. PSO for {} dataset'.format(dataset))\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "    \n",
    "interact_manual(compare_nm_pso, dataset=['iris', 'breast cancer', 'wine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-differentiable activation functions\n",
    "Some typical properties one looks for in an activation function are:\n",
    "* Non-linearity (to be able to learn arbitrary functions)\n",
    "* Monotonicity and \"Smoothness\"\n",
    "* Close approximation of the identity function near the origin (helps with learning after random initialization)\n",
    "* **Continous differentiability**\n",
    "\n",
    "The commonly used activation functions for neural networks, like $sigmoid$ and $tanh$ seen above, are differentiable in order to allow gradient-based optimization methods. A very popular activation function that is actually not differentiable (at 0) is $ReLU(x)=max(x,0)$. However, it is in fact the most popular activation function because of its simplicity and robustness to the vanishing gradient problem.\n",
    "\n",
    "Now that we are using optimization methods like the PSO, we do not need to require any differentiability and can, in fact, create arbitrary complex functions as our activations. Of course, it is nevertheless important to have functions that can be efficiently *evaluated*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step function, where return value is \n",
    "# 0 for negative, 1 for positive, and constant (chosen to be 1 here) for 0\n",
    "\n",
    "def heaviside(x):\n",
    "    return np.heaviside(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes / Food for thought\n",
    "\n",
    "* How to prevent overfitting with PSO? Early stopping can be implemented, but e.g. not low learning rate; difficult with dropout, ...\n",
    "* Nelder Mead not suitable for NN training, because code includes a lot of if and branches, i.e. not parallelizable\n",
    "* PSO can be easily parallelized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
