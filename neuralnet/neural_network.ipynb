{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "* possibly add other activation functions (non-differentiable)\n",
    "* simulated annealing? nelder mead?\n",
    "* right now, only globalbestpso used\n",
    "* add button to run optimization only after clicking\n",
    "* live plots maybe?\n",
    "* Cost history / number of iterations needed compared to classic MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks are a way of parametrizing non-linear functions. On a very basic level, they are formed by a composition of non-linear function. The function is defined with a layered architecture. The mapping from the input layer to the output layer is performed via hidden layers. Each layer $k$ produces an output $z_k$ that is a non-linear function of a weighted combination of the outputs of the previous layer, $z_k = g_k(W_k z_{k-1})$. \n",
    "\n",
    "Once the architecture and the activation functions $g_k(\\cdot)$ are defined, the weights $W_k$ are trained. If all the functions $g_k$ are (sub)-differentiable then, via the chain rule, gradients exist and can be computed. Usually, the weights are then trained via different variants of gradient descent.\n",
    "\n",
    "What we do here in this notebook is another approach: instead of using a gradient based method that does the fitting of the network, we want to apply the particle swarm optimization, simulated annealing and Nelder Mead algorithms. We will use sample datasets for binary classification that are supplied by Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact, interactive, interact_manual, fixed\n",
    "\n",
    "from utilities import plot_helpers\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (10, 5)  # Change this if figures look ugly. \n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Since the PySwarm optimizer needs a specific objective function, we implement a basic neural network with that works with different activation functions and variable hidden layers. This implementation is based on NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid aka. Logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# Relu function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# tanh\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def inv(x):\n",
    "    return np.sqrt(np.abs(x))\n",
    "\n",
    "activations = {\n",
    "    \"logistic\": sigmoid,\n",
    "    \"relu\": relu,\n",
    "    \"identity\": lambda x: x,\n",
    "    \"tanh\": tanh,\n",
    "    \"inv\": inv\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "n_features = 2\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Prop, Loss and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(p, activation, hidden_layer_sizes, X):\n",
    "    \"\"\" Calculate roll-back the weights and biases\n",
    "    Inputs\n",
    "    ------\n",
    "    p: np.ndarray\n",
    "        unrolled version of the weights and biases\n",
    "        \n",
    "    activation: function :: np.ndarray -> np.ndarray\n",
    "  \n",
    "    hidden_layer_sizes: tuple (int)\n",
    "  \n",
    "    X: np.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of logits for the output layer, which correspond\n",
    "    to the probabilities of predicting each class\n",
    "\n",
    "    \"\"\"\n",
    "    n_inputs = n_features\n",
    "    start = 0\n",
    "    in_i = X\n",
    "    for i in range(0, len(hidden_layer_sizes)):\n",
    "        layer_size = hidden_layer_sizes[i]\n",
    "        no_weights = n_inputs * layer_size\n",
    "        # get weights and biases for this layer\n",
    "        W_i = p[start:(start+no_weights)].reshape((n_inputs, layer_size))\n",
    "        b_i = p[(start+no_weights):(start+no_weights+layer_size)].reshape((layer_size,))\n",
    "        \n",
    "        out_i = activation(in_i.dot(W_i) + b_i)\n",
    "        \n",
    "        # update variables\n",
    "        start = start + no_weights + layer_size\n",
    "        n_inputs = layer_size\n",
    "        in_i = out_i\n",
    "    \n",
    "    # Compute last layer\n",
    "    no_weights = n_inputs * n_classes\n",
    "    W_k = p[start:(start+no_weights)].reshape((n_inputs, n_classes))\n",
    "    b_k = p[(start+no_weights):(start+no_weights+n_classes)].reshape((n_classes,))\n",
    "    \n",
    "    # Pre-activation\n",
    "    out = in_i.dot(W_k) + b_k\n",
    "    \n",
    "    # Probabilities using softmax\n",
    "    # make every value 0 or below, as exp(0) won't overflow\n",
    "    out_scaled = out - out.max(axis=-1, keepdims=True)\n",
    "    exp_scores = np.exp(out_scaled)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the loss of our current network\n",
    "def loss(params, activation, hidden_layer_sizes, reg, X, Y):\n",
    "    probs = forward_prop(params, activation, hidden_layer_sizes, X)\n",
    "    correct_logprobs = -np.log(probs[range(X.shape[0]), Y])\n",
    "    loss = np.sum(correct_logprobs) / X.shape[0]\n",
    "    return loss + reg * np.linalg.norm(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using the probabilities of our forward propagation\n",
    "def predict(params, activation, hidden_layer_sizes, X):\n",
    "    probs = forward_prop(params, activation, hidden_layer_sizes, X)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Demo\n",
    "\n",
    "Neural network training has a lot of hyperparameters. Architecture, learning rate, batch size, optimization algorithm, random seed are just a few of them. Additionally, we have the hyperparameters for the swarm optimization. These generally are:\n",
    "* $c_1$, the cognitive parameter (attraction towards individual best)\n",
    "* $c_2$, the social parameter (attraction towards global/neighborhood best)\n",
    "* $w$, the inertia or momentum\n",
    "\n",
    "Also, the topology can be specified when using the local optimization method (using neighborhood best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_creation(dataset, noise):\n",
    "    if dataset is 'blobs':\n",
    "        X, Y = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=3, cluster_std=10*noise)\n",
    "    elif dataset is 'circles':\n",
    "        X, Y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=noise, random_state=42)\n",
    "    elif dataset is 'moons':\n",
    "        X, Y = datasets.make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
    "    elif dataset == 'xor':\n",
    "        np.random.seed(42)\n",
    "        step = int(n_samples/4)\n",
    "        \n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([1, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = 0\n",
    "        X[3*step:4*step, :] = np.array([1, 0]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = 0\n",
    "    \n",
    "    elif dataset == 'periodic':\n",
    "        \n",
    "        step = int(n_samples/4)\n",
    "        \n",
    "        X = np.zeros((n_samples, 2))\n",
    "        Y = np.zeros(n_samples)\n",
    "        \n",
    "        X[0*step:1*step, :] = noise * np.random.randn(step, 2)\n",
    "        Y[0*step:1*step] = 1\n",
    "        X[1*step:2*step, :] = np.array([0, 2]) + noise * np.random.randn(step, 2)\n",
    "        Y[1*step:2*step] = 1\n",
    "        \n",
    "        X[2*step:3*step, :] = np.array([0, 1]) + noise * np.random.randn(step, 2)\n",
    "        Y[2*step:3*step] = 0\n",
    "        X[3*step:4*step, :] = np.array([0, 3]) + noise * np.random.randn(step, 2)\n",
    "        Y[3*step:4*step] = 0\n",
    "    \n",
    "    \n",
    "    X = X[Y <= 1, :]\n",
    "    Y = Y[Y <=1 ]\n",
    "   # Y[Y==0] = -1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the interactive function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (10, 5)  # Change this if figures look ugly. \n",
    "rcParams['font.size'] = 16\n",
    "def mlp(solver, k, dataset, hidden_layer_sizes, activation, iters, particles, c1, c2, w, reg, noise):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Initialize swarm\n",
    "    \n",
    "    # to calculate the dimension, we know we have (in * out) no. of weights\n",
    "    # and (out) no. of biases per layer\n",
    "    dimensions = 0\n",
    "    in_i = n_features\n",
    "    for l in hidden_layer_sizes:\n",
    "        dimensions += in_i * l + l\n",
    "        in_i = l\n",
    "    dimensions += in_i * n_classes + n_classes\n",
    "    print(\"Dimensions for this problem:\", dimensions)\n",
    "    \n",
    "    \n",
    "    # The options for the optimizer\n",
    "    options = {'c1': c1, 'c2': c2, 'w': w, 'k': k, 'p': 2}\n",
    "    \n",
    "    # Pick optimizer\n",
    "    if solver == 'GlobalBest':\n",
    "        optimizer = ps.single.GlobalBestPSO(n_particles=particles, dimensions=dimensions,\\\n",
    "                                        options=options)\n",
    "        \n",
    "    elif solver == 'LocalBest':        \n",
    "        optimizer = ps.single.LocalBestPSO(n_particles=particles, dimensions=dimensions,\\\n",
    "                                           options=options)\n",
    "    # get function to corresponding string\n",
    "    activation_function = activations[activation]\n",
    "    \n",
    "    # get the data\n",
    "    X, Y = data_creation(dataset, noise)\n",
    "    \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y.astype(int), test_size=.2)\n",
    "\n",
    "    # wrap our function \n",
    "    def f(x):\n",
    "        n_particles = x.shape[0]\n",
    "        res = [loss(x[i], activation_function, hidden_layer_sizes, 10**reg, X_train, y_train)\\\n",
    "               for i in range(n_particles)]\n",
    "        return np.array(res)\n",
    "    \n",
    "    cost, pos = optimizer.optimize(f, iters=iters)\n",
    "    print((y_test == predict(pos, activation_function, hidden_layer_sizes, X_test)).mean())\n",
    "    \n",
    "    \n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.figure()\n",
    "    plt.clf()\n",
    "    fig = plt.axes()\n",
    "    opt = {'marker': 'r*', 'label': '+'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 1)[0], 0], X[np.where(Y == 1)[0], 1], fig=fig, options=opt)\n",
    "    opt = {'marker': 'bs', 'label': '-'}\n",
    "    plot_helpers.plot_data(X[np.where(Y == 0)[0], 0], X[np.where(Y == 0)[0], 1], fig=fig, options=opt)\n",
    "\n",
    "    mins = np.min(X, 0)\n",
    "    maxs = np.max(X, 0)\n",
    "    x_min = mins[0] - 1\n",
    "    x_max = maxs[0] + 1\n",
    "    y_min = mins[1] - 1\n",
    "    y_max = maxs[1] + 1\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]  \n",
    "    Xplot = np.c_[XX.ravel(), YY.ravel()]   \n",
    "    \n",
    "    # get all the predictions\n",
    "    Z = forward_prop(pos, activation_function, hidden_layer_sizes, Xplot)[:,1]\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    # plt.figure(fignum, figsize=(4, 3))\n",
    "    # Put the result into a color plot\n",
    "    plt.contourf(XX, YY, Z, cmap=plt.cm.jet, alpha=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive modelation\n",
    "\n",
    "To be able to visually see and play around with the different parameters, we here give an interactive tool. One can pick different hyperparameters, datasets, layers etc. and see the dataset as well as the decision boundaries of the trained weights and biases of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c3964f0ae24531b0208fbbdc590ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Solver', options=('GlobalBest', 'LocalBest'), value='GlobalBest'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver_widget = ipywidgets.Dropdown(options=['GlobalBest', 'LocalBest'],\\\n",
    "                            value='GlobalBest', description='Solver', disabled=False)\n",
    "\n",
    "particles_widget = ipywidgets.Dropdown(options=[20,50,100,200,1000],\\\n",
    "                               value=20, description='Particles', disabled=False)\n",
    "\n",
    "k_widget = ipywidgets.IntSlider(value=5,\n",
    "                            min=1,\n",
    "                            max=particles_widget.value,\n",
    "                            step=1,\n",
    "                            readout_format='d',\n",
    "                            description='neighborhood k:',\n",
    "                            style={'description_width': 'initial'},\n",
    "                            continuous_update=False,\n",
    "                            disabled=True)\n",
    "\n",
    "def show_k(*args):\n",
    "    k_widget.disabled = True if solver_widget.value != 'LocalBest' else False\n",
    "    k_widget.max = particles_widget.value\n",
    "\n",
    "particles_widget.observe(show_k)\n",
    "solver_widget.observe(show_k)\n",
    "\n",
    "interact_manual(mlp, \n",
    "        solver=solver_widget,\n",
    "        k=k_widget,\n",
    "        dataset=['blobs', 'circles', 'moons', 'xor', 'periodic'],\n",
    "        activation=['relu', 'logistic', 'identity', 'tanh', 'inv'],\n",
    "        hidden_layer_sizes=[(50, ), (100, ), (50, 50), (100, 100), (50, 50, 50), (100, 100, 100)],\n",
    "        iters=[100,200,500,1000,5000],\n",
    "        particles=particles_widget,\n",
    "        c1=ipywidgets.FloatSlider(value=0.5,\n",
    "                                    min=0,\n",
    "                                    max=1,\n",
    "                                    step=0.1,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='c1:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "        c2=ipywidgets.FloatSlider(value=0.3,\n",
    "                                    min=0,\n",
    "                                    max=1,\n",
    "                                    step=0.1,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='c2:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "        w=ipywidgets.FloatSlider(value=0.9,\n",
    "                                    min=0,\n",
    "                                    max=1,\n",
    "                                    step=0.1,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='w:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "        reg=ipywidgets.FloatSlider(value=-3,\n",
    "                                    min=-3,\n",
    "                                    max=3,\n",
    "                                    step=0.1,\n",
    "                                    readout_format='.1f',\n",
    "                                    description='reg 10^:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),\n",
    "        noise=ipywidgets.FloatSlider(value=0.05,\n",
    "                                    min=0.01,\n",
    "                                    max=0.3,\n",
    "                                    step=0.01,\n",
    "                                    readout_format='.2f',\n",
    "                                    description='noise:',\n",
    "                                    style={'description_width': 'initial'},\n",
    "                                    continuous_update=False),  \n",
    "        );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can see that the PSO works quite well to find the fit for the classification and judging by the visualization the decision boundaries found by the swarm is very much reasonable in almost all cases. Evidently, the underlying function we are trying to optimize is an extremely high-dimensional non-convex function with (in general) no single global optimum, but many local minimas or possible solutions.\n",
    "\n",
    "Interestingly, the PSO can give somewhat different results compared to popular optimization methods like SGD and Adam. In fact, using the logistic activation function for a linearly non-separable dataset likesmoons, classic SGD fails to find a good solution, whereas PSO does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![PSO vs. SGD for logistic activation](logistic_pso_sgd.png)\n",
    "*PSO vs. SGD for logistic activation and the moon dataset, one hidden layer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "The biggest problem with PSO and neural network training certainly is the complexity. With just one hidden layer of size $50$, we have $252$ weights and biases to be adjusted, therefore a $252$-dimensional function which we then evaluate $50 * 200 = 10^4$ times for e.g. $50$ particles and $200$ iterations.\n",
    "<br/><br/>\n",
    "\n",
    "Let's consider this with a small excursion to the theoretical complexity of the forward propagation:\n",
    "\n",
    "Given one feature vector $x$, we first multiply the vector with the weight matrix $W_1$, and then add the bias $b_1$ before applying the activation function. Assuming the number of perceptrons in the hidden layer have the same dimension $n$ like the input, we have a complexity of $O(n^2)$ as a result of the matrix with vector multiplication. Repeating this for $k$ layers and $m$ data points, we get something in the order of $O(k*m*n^2)$. Repeating this operation for every particle for every iteration therefore is a big computational burden.\n",
    "<br/> <br/> <br/>\n",
    "Let's have a comparison for one specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-differentiable activation functions\n",
    "Some typical properties one looks for in an activation function are:\n",
    "* Non-linearity (to be able to learn arbitrary functions)\n",
    "* Monotonicity and \"Smoothness\"\n",
    "* Close approximation of the identity function near the origin (helps with learning after random initialization)\n",
    "* **Continous differentiability**\n",
    "\n",
    "The commonly used activation functions for neural networks, like $sigmoid$ and $tanh$ seen above, are differentiable in order to allow gradient-based optimization methods. A very popular activation function that is actually not differentiable (at 0) is $ReLU(x)=max(x,0)$. However, it is in fact the most popular activation function because of its simplicity and robustness to the vanishing gradient problem.\n",
    "\n",
    "Now that we are using optimization methods like the PSO, we do not need to require any differentiability and can, in fact, create arbitrary complex functions as our activations. Of course, it is nevertheless important to have functions that can be efficiently *evaluated*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step function, where return value is \n",
    "# 0 for negative, 1 for positive, and constant (chosen to be 1 here) for 0\n",
    "\n",
    "def heaviside(x):\n",
    "    return np.heaviside(x, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
